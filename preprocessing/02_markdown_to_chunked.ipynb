{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0a68807",
   "metadata": {},
   "source": [
    "# Markdown to Chunked Dataset\n",
    "\n",
    "This notebook converts the markdown dataset to a chunked dataset using header-based splitting. This approach preserves the document structure by keeping headings with their content, making chunks more meaningful and contextually rich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9e95219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d634fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- CONFIG ----------------\n",
    "INPUT_FILE = \"data/tawiki_markdown.parquet\"\n",
    "OUTPUT_FILE = \"data/tawiki_chunked.parquet\"\n",
    "\n",
    "# Chunking parameters\n",
    "CHUNK_SIZE = 8000          # Target chunk size in characters\n",
    "CHUNK_OVERLAP = 200        # Overlap between chunks\n",
    "\n",
    "# Headers to split on (for Tamil markdown)\n",
    "HEADERS_TO_SPLIT = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "]\n",
    "# ---------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0457d3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data/tawiki_markdown.parquet...\n",
      "Loaded 171015 documents\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n***இங்கு தமி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\\n\\n\\n\\nவி...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># விக்கிப்பீடியா:விக்கிப்பீடியர்கள்\\n\\n\\nவிக்க...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># கட்டடக்கலை\\n\\nசிறந்த மற்றும் மிகவும் நுட்பமா...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># கட்டிடங்களின் பட்டியல்\\n\\n\\nபிரபலமான அல்லது ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  # விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n***இங்கு தமி...\n",
       "1  # விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\\n\\n\\n\\nவி...\n",
       "2  # விக்கிப்பீடியா:விக்கிப்பீடியர்கள்\\n\\n\\nவிக்க...\n",
       "3  # கட்டடக்கலை\\n\\nசிறந்த மற்றும் மிகவும் நுட்பமா...\n",
       "4  # கட்டிடங்களின் பட்டியல்\\n\\n\\nபிரபலமான அல்லது ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the markdown dataset\n",
    "print(f\"Loading data from {INPUT_FILE}...\")\n",
    "df = pd.read_parquet(INPUT_FILE)\n",
    "print(f\"Loaded {len(df)} documents\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d261338d",
   "metadata": {},
   "source": [
    "## Test with a single document\n",
    "\n",
    "Let's test the chunking approach with one document to see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab664c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample document length: 3659 characters\n",
      "\n",
      "First 500 characters:\n",
      "# விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "\n",
      "\n",
      "விக்கிப்பீடியா, அதன் வாசகர்களால் ஒருமித்து எழுதப்படுகின்ற ஓர் இலவசக் கலைக்களஞ்சியம் ஆகும். **நீங்கள்** உட்பட எவரும், ஒவ்வொரு விக்கிப்பீடியா கட்டுரைப் பக்கத்திலும் காணப்படும் **தொகு** இணைப்பைச் சொடுக்குவதன் மூலம், எந்தக் கட்டுரையையும் தொகுக்க முடியும்.  \n",
      "\n",
      "## விக்கிப்பீடியாவில் உலாவுதல்\n",
      "விக்கிப்பீடியாவின் தமிழ்ப் பதிப்பு 2003இல் தான் தொடங்கப்பட்டுள்ளது எனினும், ஆங்கிலத்திலும் ஏனைய பல மொழிகளிலும், பல்வேறு துறைகளையும் சார்ந்த ஏராளமான தகவல்களை, விக்கிப்பீடியா ஏ...\n",
      "\n",
      "\n",
      "After header-based splitting: 3 sections\n",
      "\n",
      "--- Section 1 ---\n",
      "\n",
      "With headers prepended:\n",
      "# விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "விக்கிப்பீடியா, அதன் வாசகர்களால் ஒருமித்து எழுதப்படுகின்ற ஓர் இலவசக் கலைக்களஞ்சியம் ஆகும். **நீங்கள்** உட்பட எவரும், ஒவ்வொரு விக்கிப்பீடியா கட்டுரைப் பக்கத்திலும்...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Section 2 ---\n",
      "\n",
      "With headers prepended:\n",
      "# விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "## விக்கிப்பீடியாவில் உலாவுதல்\n",
      "\n",
      "விக்கிப்பீடியாவின் தமிழ்ப் பதிப்பு 2003இல் தான் தொடங்கப்பட்டுள்ளது எனினும், ஆங்கிலத்திலும் ஏனைய பல மொழிகளிலும், பல்வேறு துறைகளையும...\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Section 3 ---\n",
      "\n",
      "With headers prepended:\n",
      "# விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "## தொகுத்தல்\n",
      "\n",
      "ஒவ்வொருவரும் விக்கிப்பீடியாவிலுள்ள பக்கங்களைத் தொகுக்கமுடியும் (இந்தப் பக்கத்தையும் கூட). ஏதாவதொரு பக்கத்துக்குத் திருத்தம் தேவை என நீங்கள் கருதினால...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Test with first document\n",
    "sample_text = df['text'].iloc[1]\n",
    "print(f\"Sample document length: {len(sample_text)} characters\")\n",
    "print(f\"\\nFirst 500 characters:\\n{sample_text[:500]}...\")\n",
    "\n",
    "# Initialize splitters\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=HEADERS_TO_SPLIT,\n",
    "    strip_headers=True  # Strip headers, we'll add them back manually\n",
    ")\n",
    "\n",
    "# Split by headers first\n",
    "header_splits = markdown_splitter.split_text(sample_text)\n",
    "\n",
    "print(f\"\\n\\nAfter header-based splitting: {len(header_splits)} sections\")\n",
    "for i, doc in enumerate(header_splits[:3]):\n",
    "    print(f\"\\n--- Section {i+1} ---\")\n",
    "    # print(f\"Metadata: {doc.metadata}\")\n",
    "    # print(f\"Content preview: {doc.page_content[:200]}...\")\n",
    "    \n",
    "    # Show how headers will be prepended\n",
    "    header_prefix = \"\"\n",
    "    for j in range(1, 5):\n",
    "        header_key = f\"Header {j}\"\n",
    "        if header_key in doc.metadata:\n",
    "            header_prefix += f\"{'#' * j} {doc.metadata[header_key]}\\n\\n\"\n",
    "    # update doc.page_content to include headers\n",
    "    doc.page_content = f\"{header_prefix}{doc.page_content}\"\n",
    "    \n",
    "    print(f\"\\nWith headers prepended:\\n{doc.page_content[:200]}...\")\n",
    "    print(\"--------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edf10276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After character-level splitting: 3 chunks\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:புதுப் பயனர் பக்கம்'}\n",
      "Length: 289 characters\n",
      "Content: # விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "விக்கிப்பீடியா, அதன் வாசகர்களால் ஒருமித்து எழுதப்படுகின்ற ஓர் இலவசக் கலைக்களஞ்சியம் ஆகும். **நீங்கள்** உட்பட எவரும், ஒவ்வொரு விக்கிப்பீடியா கட்டுரைப் பக்கத்திலும்...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:புதுப் பயனர் பக்கம்', 'Header 2': 'விக்கிப்பீடியாவில் உலாவுதல்'}\n",
      "Length: 997 characters\n",
      "Content: # விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "## விக்கிப்பீடியாவில் உலாவுதல்\n",
      "\n",
      "விக்கிப்பீடியாவின் தமிழ்ப் பதிப்பு 2003இல் தான் தொடங்கப்பட்டுள்ளது எனினும், ஆங்கிலத்திலும் ஏனைய பல மொழிகளிலும், பல்வேறு துறைகளையும...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:புதுப் பயனர் பக்கம்', 'Header 2': 'தொகுத்தல்'}\n",
      "Length: 2432 characters\n",
      "Content: # விக்கிப்பீடியா:புதுப் பயனர் பக்கம்\n",
      "\n",
      "## தொகுத்தல்\n",
      "\n",
      "ஒவ்வொருவரும் விக்கிப்பீடியாவிலுள்ள பக்கங்களைத் தொகுக்கமுடியும் (இந்தப் பக்கத்தையும் கூட). ஏதாவதொரு பக்கத்துக்குத் திருத்தம் தேவை என நீங்கள் கருதினால...\n"
     ]
    }
   ],
   "source": [
    "# Apply character-level splitting for size constraint\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP\n",
    ")\n",
    "\n",
    "# Split the header-based sections into smaller chunks\n",
    "final_chunks = text_splitter.split_documents(header_splits)\n",
    "\n",
    "print(f\"After character-level splitting: {len(final_chunks)} chunks\")\n",
    "print(f\"\\nSample chunks:\")\n",
    "for i, chunk in enumerate(final_chunks[:3]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(f\"Metadata: {chunk.metadata}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0d410b",
   "metadata": {},
   "source": [
    "## Process all documents\n",
    "\n",
    "Now let's process all documents and create the chunked dataset with metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3acd1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(text):\n",
    "    \"\"\"\n",
    "    Process a single document: split by headers, then by character limit.\n",
    "    Returns list of dicts with text and metadata.\n",
    "    Headers are prepended BEFORE character-level splitting for better context.\n",
    "    \"\"\"\n",
    "    # Initialize splitters\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "        headers_to_split_on=HEADERS_TO_SPLIT,\n",
    "        strip_headers=True  # Strip from content, we'll add them back manually\n",
    "    )\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP\n",
    "    )\n",
    "    \n",
    "    # Split by headers first\n",
    "    try:\n",
    "        header_splits = markdown_splitter.split_text(text)\n",
    "    except Exception as e:\n",
    "        # If header splitting fails, treat entire text as one document\n",
    "        from langchain_core.documents import Document\n",
    "        header_splits = [Document(page_content=text, metadata={})]\n",
    "    \n",
    "    # Prepend headers to each section BEFORE character splitting\n",
    "    from langchain_core.documents import Document\n",
    "    sections_with_headers = []\n",
    "    \n",
    "    for doc in header_splits:\n",
    "        # Build header prefix from metadata\n",
    "        header_prefix = \"\"\n",
    "        for i in range(1, 5):  # Header 1 to Header 4\n",
    "            header_key = f\"Header {i}\"\n",
    "            if header_key in doc.metadata:\n",
    "                header_prefix += f\"{'#' * i} {doc.metadata[header_key]}\\n\\n\"\n",
    "        \n",
    "        # Prepend headers to content\n",
    "        text_with_headers = header_prefix + doc.page_content\n",
    "        \n",
    "        # Create new document with headers in content\n",
    "        sections_with_headers.append(\n",
    "            Document(page_content=text_with_headers, metadata=doc.metadata)\n",
    "        )\n",
    "    \n",
    "    # Now split by character limit (headers are already in the content)\n",
    "    final_chunks = text_splitter.split_documents(sections_with_headers)\n",
    "    \n",
    "    # Convert to dict format\n",
    "    result = []\n",
    "    for chunk in final_chunks:\n",
    "        result.append({\n",
    "            'text': chunk.page_content,\n",
    "            'metadata': chunk.metadata\n",
    "        })\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6c7d6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 171015 documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/171015 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 171015/171015 [02:21<00:00, 1206.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total chunks created: 532233\n",
      "DataFrame shape: (532233, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td># விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n***இங்கு தமி...</td>\n",
       "      <td>{'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td># விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## முந்தைய க...</td>\n",
       "      <td>{'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td># விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## பயனர் கரு...</td>\n",
       "      <td>{'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td># விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## பகுப்புத்...</td>\n",
       "      <td>{'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td># விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## மலேசியாவி...</td>\n",
       "      <td>{'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  # விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n***இங்கு தமி...   \n",
       "1  # விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## முந்தைய க...   \n",
       "2  # விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## பயனர் கரு...   \n",
       "3  # விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## பகுப்புத்...   \n",
       "4  # விக்கிப்பீடியா:கலந்துரையாடல்\\n\\n## மலேசியாவி...   \n",
       "\n",
       "                                            metadata  \n",
       "0       {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்'}  \n",
       "1  {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...  \n",
       "2  {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...  \n",
       "3  {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...  \n",
       "4  {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', '...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all documents\n",
    "all_chunks = []\n",
    "\n",
    "print(f\"Processing {len(df)} documents...\")\n",
    "for text in tqdm(df['text']):\n",
    "    chunks = process_document(text)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Create DataFrame\n",
    "chunks_df = pd.DataFrame(all_chunks)\n",
    "print(f\"DataFrame shape: {chunks_df.shape}\")\n",
    "chunks_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934b9285",
   "metadata": {},
   "source": [
    "## Analyze chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6155835f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk statistics:\n",
      "Total chunks: 532233\n",
      "\n",
      "Text length (characters):\n",
      "count    532233.000000\n",
      "mean        692.793070\n",
      "std         956.501444\n",
      "min           5.000000\n",
      "25%         241.000000\n",
      "50%         437.000000\n",
      "75%         758.000000\n",
      "max        7999.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Word count:\n",
      "count    532233.000000\n",
      "mean         90.099361\n",
      "std         187.534264\n",
      "min           2.000000\n",
      "25%          29.000000\n",
      "50%          51.000000\n",
      "75%          89.000000\n",
      "max        3244.000000\n",
      "Name: word_count, dtype: float64\n",
      "\n",
      "Chunks with header metadata: 532233 (100.0%)\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "chunks_df['text_length'] = chunks_df['text'].str.len()\n",
    "chunks_df['word_count'] = chunks_df['text'].str.split().str.len()\n",
    "\n",
    "print(\"Chunk statistics:\")\n",
    "print(f\"Total chunks: {len(chunks_df)}\")\n",
    "print(f\"\\nText length (characters):\")\n",
    "print(chunks_df['text_length'].describe())\n",
    "print(f\"\\nWord count:\")\n",
    "print(chunks_df['word_count'].describe())\n",
    "\n",
    "# Check how many chunks have header metadata\n",
    "chunks_with_headers = chunks_df[chunks_df['metadata'].apply(lambda x: len(x) > 0)]\n",
    "print(f\"\\nChunks with header metadata: {len(chunks_with_headers)} ({len(chunks_with_headers)/len(chunks_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "989e2822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample chunks with metadata:\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Chunk 1\n",
      "================================================================================\n",
      "Length: 571 chars, 53 words\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்'}\n",
      "\n",
      "Text:\n",
      "# விக்கிப்பீடியா:கலந்துரையாடல்\n",
      "\n",
      "***இங்கு தமிழ் விக்கிப்பீடியாவைப் பற்றிய உங்கள் பொதுவான கருத்துக்கள், பாராட்டுக்கள் மற்றும் ஆலோசனைகளைத் தெரிவிக்கலாம். உங்கள் கருத்துகளுக்கு மற்ற விக்கிப்பீடியர்கள் பதில் அளிப்பார்கள். தகுந்த ஆலோசனைகளை உடனே செயற்படுத்தவும் செய்வோம். விக்கிப்பீடியா திட்டத்தின் வளர்ச்சி...\n",
      "\n",
      "================================================================================\n",
      "Chunk 2\n",
      "================================================================================\n",
      "Length: 206 chars, 22 words\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', 'Header 2': 'முந்தைய கலந்துரையாடல்கள்'}\n",
      "\n",
      "Text:\n",
      "# விக்கிப்பீடியா:கலந்துரையாடல்\n",
      "\n",
      "## முந்தைய கலந்துரையாடல்கள்\n",
      "\n",
      "- **தொகுப்பு 01** (மயூரநாதனின் முதற் குறிப்பு!, தமிழ் விக்கிப்பீடியா 1000 கட்டுரைகள்.)  \n",
      "- **தொகுப்பு 02**\n",
      "மிகவும் அருமையான முயற்சி வாழ்த்துக்கள்...\n",
      "\n",
      "================================================================================\n",
      "Chunk 3\n",
      "================================================================================\n",
      "Length: 2372 chars, 257 words\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', 'Header 2': 'பயனர் கருத்துகள்'}\n",
      "\n",
      "Text:\n",
      "# விக்கிப்பீடியா:கலந்துரையாடல்\n",
      "\n",
      "## பயனர் கருத்துகள்\n",
      "\n",
      "உங்கள் கருத்துகளை இதன் கீழ் இடவும். கையெழுத்து இடப் பார்க்கவும்: விக்கிப்பீடியா:கையெழுத்து  \n",
      "First time I saw the Tamil version of Wikimedia site. It's great...keep it up. Unfortunately I don't know how to use the tamil font here. All the best. --...\n",
      "\n",
      "================================================================================\n",
      "Chunk 4\n",
      "================================================================================\n",
      "Length: 504 chars, 51 words\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', 'Header 2': 'பகுப்புத் தலைப்புகள்'}\n",
      "\n",
      "Text:\n",
      "# விக்கிப்பீடியா:கலந்துரையாடல்\n",
      "\n",
      "## பகுப்புத் தலைப்புகள்\n",
      "\n",
      "- கரையோர வேடர்கள் என்னும் கட்டுரைக்கு, கரையோர வேடர்கள் என்னும் பகுப்புத் தலைப்பு உருவாக்கப்பட்டிருந்தது. அதனை உருவாக்கிய அறிஞரே காட்டியிருந்த இலங்கைத் தமிழர் என்னும் பகுப்புத் தலைப்புக்கு உட்படுத்தியிருக்கிறேன். தெரிந்திருந்தும் பழைய பகுப்பில்...\n",
      "\n",
      "================================================================================\n",
      "Chunk 5\n",
      "================================================================================\n",
      "Length: 2945 chars, 314 words\n",
      "Metadata: {'Header 1': 'விக்கிப்பீடியா:கலந்துரையாடல்', 'Header 2': 'மலேசியாவின் மலாய்ப் பெயர்கள்'}\n",
      "\n",
      "Text:\n",
      "# விக்கிப்பீடியா:கலந்துரையாடல்\n",
      "\n",
      "## மலேசியாவின் மலாய்ப் பெயர்கள்\n",
      "\n",
      "வணக்கம். மலேசியாவில் மாநிலங்கள், நகரங்களின் பெயர்களை மலாய் மொழியில் எப்படி அழைக்கிறார்களோ, அப்படி அழைப்பதுதான் நல்லது. Penang என்பதை பினாங்கு என்று அழைப்பதைப் போல Pahang என்பதை பகாங்கு என்று அழைக்கமாட்டார்கள். Perak என்பதை பேராக்கு என்...\n"
     ]
    }
   ],
   "source": [
    "# Display sample chunks with their metadata\n",
    "print(\"Sample chunks with metadata:\\n\")\n",
    "for i in range(min(5, len(chunks_df))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Chunk {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Length: {chunks_df.iloc[i]['text_length']} chars, {chunks_df.iloc[i]['word_count']} words\")\n",
    "    print(f\"Metadata: {chunks_df.iloc[i]['metadata']}\")\n",
    "    print(f\"\\nText:\\n{chunks_df.iloc[i]['text'][:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9960e1d5",
   "metadata": {},
   "source": [
    "## Save chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38795632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 532233 chunks to data/tawiki_chunked.parquet\n",
      "Also saved to data/tawiki_chunked.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Drop temporary columns before saving\n",
    "chunks_df_final = chunks_df.drop(columns=['text_length', 'word_count'])\n",
    "\n",
    "# Save as parquet\n",
    "chunks_df_final.to_parquet(OUTPUT_FILE, index=False)\n",
    "print(f\"Saved {len(chunks_df_final)} chunks to {OUTPUT_FILE}\")\n",
    "\n",
    "# Also save as JSON for inspection\n",
    "json_output = OUTPUT_FILE.replace('.parquet', '.jsonl')\n",
    "chunks_df_final.to_json(json_output, orient='records', lines=True, force_ascii=False)\n",
    "print(f\"Also saved to {json_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
